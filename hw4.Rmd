---
title: "SOC577 Homework 4: Machine learning"
author: "Your name here"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
# Do not edit this chunk

# The following lines define how the output of code chunks should behave
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(include = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(error = TRUE)

# Required packages, please install any you do not have
library(rmarkdown)
library(tidyverse)
library(knitr)
library(stringr)
library(tidytext)
library(ggplot2)
library(viridis)
library(tidymodels)
library(textrecipes)
library(glmnet)

set.seed(148508901)
```

# Instructions

This assignment is designed to build your familiarity with the machine techniques covered in class. As in the previous assignments, it will involve a combination of short written answers and coding in R. All answers should be written in this document. *Please write answers to written questions outside of the code cells rather than as comments.*

### Requirements
You should be viewing this document in RStudio. If you have not done so already, make sure to install the required packages (see initial chunk). You can do this by clicking the ``Install`` button in the Packages tab in the lower-right corner of RStudio and following the directions on the installation menu. You can also install packages by entering ``install.packages(x)`` into the R Console, where ``x`` is the name of the package.

# Predicting political party from tweets

## Loading the data
We're going to be working with the Twitter politics dataset you used in the previous homework. This time you will be attempting to predict whether a tweet is written by a Democrat or a Republican.
```{r loading data, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
data <- read_csv("data/politics_twitter.csv") %>% select(screen_name, text)
data$party <- ifelse(data$screen_name %in% c("JoeBiden", "KamalaHarris", "SpeakerPelosi", "BernieSanders", "AOC", "SenSchumer"),
                     "Democrat", "Republican")
data <- data %>% 
    mutate(text = gsub("#[A-Za-z0-9]+|@[A-Za-z0-9]", "", text)) %>% # Removing hashtags and mentions
    mutate(text = gsub("(http[^ ]*)|(www.[^ ]*)", "", text)) %>% # Removing URLs
    distinct(text, .keep_all =TRUE)
```

## Questions

Q1. Before doing any modeling, exanube whether there are any differences between the tweets by Republicans and Democrats with respect to how much they tweet. In the cell below, write a line of code to calculate the total number of tweets written by each group.
```{r q1, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}

```

To make it a fair prediction task, we can take identically sized random samples from each group. Given the 50:50 class distribution, our baseline is a random guess. Run the chunk below then proceed.
```{r sampling, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
rep.sample <- sample_n(data %>% filter(party == "Republican"), size=2000)
dem.sample <- sample_n(data %>% filter(party == "Democrat"), size=2000)
data <- bind_rows(rep.sample, dem.sample)
```

Q2. Now that we have our dataset, we can start to construct the modeling pipeline. The first step is to take a test-train split. Add arguments to `initial_split` to create a split where 20% of the data are held-out for testing and the classes are evenly balanced across test and training sets
```{r q2, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
data_split <- initial_split()
train <- training(data_split)
test <- testing(data_split)
```

Q3. Now we want to put together a recipe. The first line specifies that we are modeling the party as a function of the text using the training data. Add the following steps from the `textrecipes` package (in order):

  - Tokenize
  - Remove stopwords
  - Add N-grams from length 1 to 3
  - Filter to retain the 1000 most frequent n-grams
  - Construct TF-IDF matrix

You can use `prep` and `bake` to run this process and view the resulting feature matrix.
```{r q3, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
party_recipe <- recipe(party ~ text, data  = train) %>% 
    # Add your steps here

# Prints feature matrix example (do not modify)
head(prep(party_recipe, train) %>% bake(test))
```

Q4. Let's add a model and put together a workflow. We will use a logistic regression with a LASSO penalty. Add the recipe and the model to the workflow `wf` then answer the question below.
```{r q4, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
# Do not modify the model
lasso <- logistic_reg(penalty = 0.01, mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

wf <- workflow() %>% 
  # Add your recipe and model
```
What is the purpose of using a workflow?
Answer:

Q5. We will use 5-fold cross-validation to evaluate performance on the training data. Modify the `vfold_cv` function to ensure that each fold has a balanced class distribution.

Next, run the rest of the chunk to fit the model to each fold and calculate statistics. This may take a couple of minutes to complete. Answer the question below.
```{r q5, echo=TRUE, tidy=TRUE}
folds <- vfold_cv(train, v = 5) # Add an argument for balance

# Do not modify code below
fitted <- fit_resamples(
  wf,
  folds,
  control = control_resamples(save_pred = TRUE),
  metrics = metric_set(precision, recall, f_meas, roc_auc)
)
```
Why do we want to stratify the balance the class distribution in each fold?
Answer: 

Q6. We can now get the predictions from the model and conduct some analyses of the results. Run these lines then answer the question below.
```{r q6, echo=TRUE, tidy=TRUE}
collect_metrics(fitted)
```
Interpret the precision and recall metrics.
Answer:


Q7. Let's try to perform the classification using an alternative model. Specifically, using a single-layer neural network from the `keras` package. The documentation for `parsnip` (part of `tidymodels`) explains how to implement a neural network using the `mlp` function: https://parsnip.tidymodels.org/reference/mlp.html

Complete the code below to specify the model and parameter grid. The model should have the following components: 

    - The number of hidden units and dropout should be tunable.
    - For the number of hidden units, test three different values between 10 and 100. 
    - For dropout, evaluate models with values of 0 and 0.05. 
    - The training epochs should be fixed to 75. 
    - No additional arguments need to be specified.

Once you have completed the code, answer the questions below then execute the chunk afterwards to run the models.
```{r q7a, echo=TRUE, tidy=TRUE}
# install.packages("keras") # Uncomment and run to install keras. Remove before knitting final submission
# Note: `keras` does not work for some users. Try `brulee` instead. Replace package name below and in `set_engine` call. The first time `brulee` is loaded respond "Yes" to the question in the console to download package files.
library(keras)

# Specify the arguments to the `mlp` function to set model parameters
neural_network <- mlp() %>%
  set_mode("classification") %>%
  set_engine("keras") 

# Specify a parameter grid
param_grid <- grid_regular(
  # Add parameter and range
  # Add parameter and range
  # Specify parameter levels
)

print(param_grid)
```
What is the purpose of the dropout parameter?
Answer:

What is an epoch?
Answer:

In total, how many combinations of parameters are being analyzed?
Answer:

Taking the cross-validation into account, how many models will be estimated in this process?
Answer:

Run the chunk below to add the model to the work flow and execute the code. As the code runs, you will see that R prints out the loss score at each epoch, representing how the model is fitting the data. It will also make a series of plots. You should see that the loss decreases over time, indicating that the model is learning to predict the outcome more accurately, although it can also fluctuate up and down as the model tests different weights.

This code will take around 10-15 minutes to run since we need to fit several different models, so you might want to go for a walk or make a coffee. Note that it will also take a while to knit the document when you generate the final version since this process will be repeated.
```{r q7b, echo=TRUE, tidy=TRUE}
# Do not modify
wf <- wf %>% update_model(neural_network)

gridsearch <- tune_grid(
  wf,
  folds,
  grid = param_grid,
  metrics = metric_set(precision, recall, f_meas, roc_auc),
  control = control_resamples(save_pred = TRUE)
)
```


Q8. This plot shows how the different hyperparameter combinations affect performance. Run the chunk and answer the questions below.
```{r q8, echo=TRUE, tidy=TRUE}
# Do not modify
autoplot(gridsearch) + 
  labs(title = "Model performance across regularization strength and type",
  color = "dropout") + scale_color_viridis_d() + theme_classic()
```
Analyze the graph above and describe the effects of varying the two parameters. Is there an optimal combination of parameters? Are there any trade-offs?
Answer:

Q9.  Use `select_best()` to find the best performing model according to the ROC-AUC measure. Next, filter the results from `collect_metrics` to show the results for the best model. The table should have four rows. Inspect the F1, precision, and recall then answer the question below.
```{r q9, echo=TRUE, tidy=TRUE}
best_params <- #
collect_metrics(gridsearch) %>% # Complete the pipe
```
How does the model perform on each of the four metrics compared to the logistic regression used above (output from Q6)? Does it perform better or worse overall?
Answer: 

Q10. Run the code below to take the best parameters and estimate a final model. Proceed to the next chunk once the code has finished.
```{r q10a, echo=TRUE, tidy=TRUE}
# Do not modify
final_wf <- finalize_workflow(wf, best_params)
final_model <- last_fit(final_wf, data_split)
```

Execute this chunk to calculate the performance of the model on the out-of-sample test data then answer the questions below.
```{r q10b, echo=TRUE, tidy=TRUE}
# Do not modify
final.precision <- collect_predictions(final_model) %>% precision(truth=party, estimate = .pred_class)
final.recall <- collect_predictions(final_model) %>% recall(truth=party, estimate = .pred_class)
final.f1 <- collect_predictions(final_model) %>% f_meas(truth=party, estimate = .pred_class)
print(bind_rows(final.precision, final.recall, final.f1))
```
Does the model perform better or worse on the test data compared with the training data?
Answer:

Do these results imply that the model has underfit or overfit the training data?
Answer:

Is this task more or less difficult than you expected? Are there factors that might make it difficult to predict political affiliation from tweets?
Answer:

*This is the end of the assignment. Please submit it following the instructions below.*

### AI usage
Please use this space to document any usage of AI tools (e.g. ChatGPT) during this assignment:

### Submitting the homework
Once you have finished the assignment please complete the following steps to submit it:

1. Click on the ``Knit`` menu at the top of the screen and select ``Knit to HTML``. This will execute the all of the code and render the RMarkdown document in HTML. Verify that this document contains all of your answers and that none of the chunks produce error messages. It will take a while to knit since the models must be reestimated.
2. Add this document *and* the HTML file to Github. Use ``Homework submitted`` as your main commit message.
3. Push the commit to Github.
4. Visit the Github repository in your browser and verify that the final version of both files has been correctly uploaded.
