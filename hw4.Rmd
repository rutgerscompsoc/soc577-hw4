---
title: "SOC 577 Homework 4: Machine learning"
author: "Your name here"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
# Do not edit this chunk

# The following lines define how the output of code chunks should behave
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(include = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(error = TRUE)

# Required packages, please install any you do not have
library(rmarkdown)
library(tidyverse)
library(knitr)
library(stringr)
library(tidytext)
library(ggplot2)
library(viridis)
library(parallel)
library(tidymodels)
library(textrecipes)
library(glmnet)
library(discrim)
library(kernlab)
library(LiblineaR)
```

# Instructions

This assignment is designed to build your familiarity with the machine techniques covered in class. As in the previous assignments, it will involve a combination of short written answers and coding in R. All answers should be written in this document. *Please write answers to written questions outside of the code cells rather than as comments.*

### Requirements
You should be viewing this document in RStudio. If you have not done so already, make sure to install the required packages (see initial chunk). You can do this by clicking the ``Install`` button in the Packages tab in the lower-right corner of RStudio and following the directions on the installation menu. You can also install packages by entering ``install.packages(x)`` into the R Console, where ``x`` is the name of the package.

### Submitting the homework
Once you have finished the assignment please complete the following steps to submit it:

1. Click on the ``Knit`` menu at the top of the screen and select ``Knit to HTML``. This will execute the all of the code and render the RMarkdown document in HTML. Verify that this document contains all of your answers and that none of the chunks produce error messages.
2. Add this document *and* the HTML file to Github. Use ``Homework submitted`` as your main commit message.
3. Push the commit to Github.
4. Visit the Github repository in your browser and verify that the final version of both files has been correctly uploaded.

# **Predicting shows and characters**
# Loading the data
We're going to be working with both datasets from the previous assignment. The dataset contains every utterance by the main characters in Friends and South Park (see `hw4_create_dataset.R` for the code used to construct it). It contains 51,047 lines from Friends and 25,434 from South Park. I suspect this is because there are far more characters in South Park.
```{r loading data, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
data <- read_tsv("friends_or_southpark.tsv") %>% select(speaker,text,show)
data %>% group_by(show) %>% summarise(n())
```

# Questions

Q1. Before doing any modeling we should conduct some comparisons of the two shows and to filter out very short lines.

Complete the `mutate` function below to create new columns indicating the number of character and words in each line.

Analyze the results of the two `t.tests`. Do you notice any differences between the two shows? Do you think these differences might impact the results?
```{r q1, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
data$text <- str_remove_all(data$text, '\n') # Remove newline characters

# Create columns for num_chars and word_count
data <- data %>% mutate(n_chars = , 
                        n_words = )

# T-tests to assess if there are differences in character and word counts
print(t.test(n_chars ~ show, data = data))
print(t.test(n_words ~ show, data = data))

data <- data %>% filter(n_words > 2) # Do not modify, files out lines less than two words
```

We will begin with a simple binary prediction task. How well can we predict which show a given line was from? To make this a fairer comparison we will take a random sample the same number lines from each of the two shows. 

Given the 50:50 class distribution, our baseline is a random guess. How much better can we predict the show given the line than a coin toss?

```{r sampling, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
set.seed(08901)
sp.sample <- sample_n(data %>% filter(show == "South Park"), size=2000)
fr.sample <- sample_n(data %>% filter(show == "Friends"), size=2000)
data <- bind_rows(sp.sample, fr.sample)
```

Q2. Now that we have our dataset, we can start to construct the modeling pipeline. The first step is to take a test-train split. Add arguments to `initial_split` to create a split where 10% of the data are held-out for testing and the classes are evenly balanced across test and training sets
```{r q2, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
data_split <- initial_split()
train <- training(data_split)
test <- testing(data_split)
```

Q3. Now we want to put together a recipe. The first line specifies that we are modeling the show as a function of the text using the training data. Add the following steps from the `textrecipes` package:

  - Tokenize
  - Remove stopwords
  - Stem
  - Add N-grams from length 1 to 3 (you will have to use `step_untokenize` first)
  - Filter 1000 most frequent tokens
  - Construct TF-IDF matrix

You can use `prep` and `bake` to run this process and view the resulting feature matrix.
```{r q3, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
show_recipe <- recipe(show ~ text, data  = train) %>% 
    # Add your steps here

head(prep(show_recipe, train) %>% bake(test)) ## Run to view feature matrix after preprocessing
```

Q4. Let's add a model and put together a workflow. We will use a logistic regression with a LASSO penalty. Add the recipe and the model to the workflow `wf` then answer the question below.
```{r q4, echo=FALSE, tidy=TRUE, eval=TRUE, include=FALSE}
lasso <- logistic_reg(penalty = 0.01, mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

wf <- workflow() %>% 
  # Add your recipe and model
```
What is the purpose of using a workflow?

Q5. We will use 5-fold cross-validation to evaluate performance on the training data. Modify the `vfold_cv` function to ensure that each fold has a balanced class distribution.

Next, run the rest of the chunk to fit the model to each fold and calculate statistics. This may take a couple of minutes to run. Answer the question below.
```{r q5, echo=TRUE, tidy=TRUE}
folds <- vfold_cv(train, v = 5) # Add an argument

fitted <- fit_resamples(
  wf,
  folds,
  control = control_resamples(save_pred = TRUE),
  metrics = metric_set(precision, recall, f_meas, roc_auc)
)
```
Why do we want to stratify the balance the class distribution in each fold?

Q6. We can now get the predictions from the model and conduct some analyses of the results. Run these lines then answer the question below.
```{r q6, echo=TRUE, tidy=TRUE}
pred_probs <- collect_predictions(fitted, type = "prob")

collect_metrics(fitted)
```
What do these metrics tell us about the performance of the classifier?

Q7. In the previous example, and in the lecture on text classification, we have considered models trained on sparse representations. Let's examine how the model might perform if we have a dense representation of each text. In this case we will use a pre-trained word embedding (the same one as lecture 8).

`embedding.tsv` is a tibble containing the subset of the word vectors corresponding to the tokens in this dataset. You can see how it was created by viewing `hw4_custom_embedding.R`.

Modify the recipe to use the embedding as the final stage. Look up the documentation for the appropriate step. Each document should be represented as the average over the embeddings of its constituent words. Note that we the stemming step has been removed as this would prevent us from matching words to their embeddings. There is also no need to add N-grams. Once you have modified the recipe you then need to update the recipe used in the workflow.

Finally, run the remaining steps to train the model and view performance metrics. This may also take a couple of minutes.
```{r q7, echo=TRUE, tidy=TRUE}
# Loading embeddings
emb <- read_tsv('embedding.tsv') %>% as_tibble()

show_recipe_dense <- recipe(show ~ text, data  = train) %>% 
    step_tokenize(text) %>%
    step_stopwords(text) %>%
    step_tokenfilter(text, min_times = 3) 
    # Add the final step here    

wf <- wf %>% # Update the workflow

fitted.2 <- fit_resamples(
  wf,
  folds,
  control = control_resamples(save_pred = TRUE),
  metrics = metric_set(precision, recall, f_meas, roc_auc)
)

pred_probs.2 <- collect_predictions(fitted.2, type = "prob")

collect_metrics(fitted.2)
```

Q8. We can better compare the models by looking at the respective ROC curves by fold. Please run the chunk and answer the questions below.
```{r q8, echo=TRUE, tidy=TRUE}
pred_probs$model <- "TF-IDF"
pred_probs.2$model <- "word2vec"
preds <- bind_rows(pred_probs, pred_probs.2)

preds %>%
  group_by(id, model) %>%
  roc_curve(truth = show, .pred_Friends) %>%
  autoplot() +
  aes(linetype=model) +
  labs(
    color = NULL,
    title = "ROC curve",
    y = "True positive rate",
    x = "False positive rate"
  ) + scale_color_viridis_d()
```
How do the performance metrics calculated at the end of the previous question compare to those of the original model? What do you observe in the ROC plot? Overall, which model better fits the data?

Q9. One of the reasons this model might not perform as well is that LASSO regression works best with sparse data. For dense representations we should prefer an L2-penalty. We can modify the type of penalty by using the `mixture` parameter. 

Let's experiment with these parameters by varying `mixture` and analyzing the results. In addition, we should vary the `penalty` parameter to find an optimal value. Specify a new model with the tunable parameters, then construct a parameter grid (see specific values in the comment below).

Then run the remainder of the chunk to tune the model. This will take a few minutes since we need to fit several different models.
```{r q9, echo=TRUE, tidy=TRUE}
l2 <- # Specify the logistic regression with tunable parameters

# penalty can range from 0.001 to 1, mixture from 0 to 1. 
# Test four different values in each range for each parameter
param_grid <- 

wf <- wf %>% update_model(l2)

fitted.3 <- tune_grid(
  wf,
  folds,
  grid = param_grid,
  metrics = metric_set(precision, recall, f_meas, roc_auc),
  control = control_resamples(save_pred = TRUE)
)

pred_probs.3 <- collect_predictions(fitted.3, type = "prob")
```


Q10. We can plot the results to assess how the different hyperparameters affect performance. Run the chunk and answer the questions below.
```{r q10, echo=TRUE, tidy=TRUE}
autoplot(fitted.3) + 
  labs(title = "Model performance across regularization strength and type",
  color = "mixture") + scale_color_viridis_d()
```
Analyze the graph above and describe the effects of varying the penalty and mixture. 

Based on these results, do you think the dense or sparse representation is preferable?

Do you have any ideas into what might be leading to differences in performance between the two different models?

Q12. So far we have seen how we can use these methods to distinguish between lines from Friends and South Park with varying degrees of accuracy. Let's try a slightly different task for the final part of the assignment. How well can you distinguish between different characters? 

Use the tools you have learned to specify a new recipe, model, and workflow for character prediction. You may use any type of feature representation and model (you will have to use a model that allows for multiclass outcomes). I encourage you to test different specifications to see what works best.

You do not need to redefine the train/test split, but you should create new set of k-folds that are stratified by speaker.
```{r q12, echo=TRUE, tidy=TRUE}
folds.char <- vfold_cv() # Complete arguments

char_recipe <- recipe(speaker ~ text, data  = train) %>%
    # Add recipe steps

m <- # Add a model

wf.char <- workflow() %>% add_recipe(char_recipe) %>% add_model(m)

fitted.char <- fit_resamples(
  wf.char,
  folds.char,
  control = control_resamples(save_pred = TRUE),
  metrics = metric_set(precision, recall, f_meas, roc_auc)
)

collect_metrics(fitted.char)
```
Discuss the results. How well does the classifier perform at the task? Did you find different features or classifiers tended to work better than others? 

Note that the statistics are now averages across each class so the interpretation is slightly different to the binary case. For example, the precision score for Cartman will be different to the precision score for Kyle.


Q.13. When you are satisfied with the model above, run the chunk below to fit the workflow to the entire dataset. The second line will then construct a confusion matrix. This code will take a few minutes to run.

When the model has finished, answer the question below.
```{r q13, echo=TRUE, tidy=TRUE}
final_fitted <- last_fit(wf.char, data_split)

collect_predictions(final_fitted) %>%
  conf_mat(truth = speaker, estimate = .pred_class) %>%
  autoplot(type = "heatmap") + scale_fill_viridis_c()
```

Interpret the results of the confusion matrix. 

Are the any characters we can predict better than others? 

Are there any characters we tend to frequently mistake for others? 

Do the results appear to vary much by show or character?

*This is the end of the assignment. Please submit it following the instructions at the beginning of this document.*